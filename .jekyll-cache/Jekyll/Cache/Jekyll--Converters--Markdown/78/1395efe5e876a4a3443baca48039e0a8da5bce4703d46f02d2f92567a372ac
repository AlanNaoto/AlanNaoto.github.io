I"x<h1 id="exploring-varied-object-detection-architectures">Exploring varied object detection architectures</h1>

<div class="card">
    <div class="iframe-container">
        <iframe src="https://www.youtube.com/embed/gEMXanC8I6w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div>    
<br />
<a href="https://github.com/AlanNaoto/carla-dataset-runner"> <img src="assets/imgs/GitHub-Mark-64px.png" width="5%" />
<b>Take a peek into the source code :hugs: </b></a>
</div>

<p>When deploying machine learning models to the real world, specially in resource constrained environments, 
then model size and complexity plays a big role into determining it’s usability, i.e.: will it make
predictions that are correct and also in a reasonable time?</p>

<p>In order to have an idea of the impact that your hardware has on the object detection task, I’m presenting to you
a benchmark table on a set of 1600 JPEG frames with 1280x720 resolution on the following hardware:</p>

<table>
  <thead>
    <tr>
      <th>Hardware</th>
      <th>CPU</th>
      <th>RAM</th>
      <th>Graphics card</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Raspberry Pi 3 B+</td>
      <td>1.4GHz 64-bit quad-core ARM Cortex-A53 CPU</td>
      <td>1GB LPDDR2 SDRAM</td>
      <td>None</td>
    </tr>
    <tr>
      <td>Raspberry Pi 3 B+</td>
      <td>1.4GHz 64-bit quad-core ARM Cortex-A53 CPU</td>
      <td>1GB LPDDR2 SDRAM</td>
      <td>Movidius Neural Compute Stick (not a video card, but helps for GPU processing)</td>
    </tr>
    <tr>
      <td>Dell G3 15</td>
      <td>Intel(R) Core(TM) i5-8300H CPU @ 2.30GHz</td>
      <td>8GB SODIMM DDR4 Synchronous</td>
      <td>GeForce GTX 1050 Mobile</td>
    </tr>
  </tbody>
</table>

<p>And the results:</p>

<p>| Algorithm | Model size | FPS |
|———-|—–|—–|————|</p>

:ET